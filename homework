# Exercise 1
from sklearn.datasets import fetch_20newsgroups

categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']
newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)

for i in range(3):
    print(f"Sample {i+1}:")
    print(newsgroups_train.data[i])
    print("-" * 80)



#Exercise 2 
import pandas as pd

df = pd.DataFrame(newsgroups_train.data, columns=["text"])
df["target"] = newsgroups_train.target

atheism_data = df[df["target"] == newsgroups_train.target_names.index('alt.atheism')]
print(atheism_data.head())



#Exercise 3
import pandas as pd
from sklearn.datasets import fetch_20newsgroups

categories = ['sci.med']
newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)

df = pd.DataFrame(newsgroups_train.data, columns=["text"])
df["target"] = newsgroups_train.target

sci_med_data = df[df["target"] == newsgroups_train.target_names.index('sci.med')]

every_10th_record = sci_med_data.iloc[::10].head(5)
print(every_10th_record)

#Exercise 4 
import pandas as pd
import numpy as np

data = {
    'A': [1, np.nan, 3, np.nan],
    'B': [4, np.nan, np.nan, 8],
    'C': [np.nan, 6, 7, np.nan]
}
df = pd.DataFrame(data)

missing_values_per_record = df.isnull().sum(axis=1)
print(missing_values_per_record)

#Exercise 5
.isnull() specifically detects:
np.nan
None

This function doesn’t automatically handle other placeholder strings or empty strings ('') as missing values because they don’t match NaN or None values. As a result:

Row C ('NaN') and row D ('None') are treated as strings rather than nulls.
The empty string in row F ('') is also not detected as missing.




#Exercise 6 




#Exercise 7

import matplotlib.pyplot as plt
import numpy as np

x = np.arange(0, 10, 0.1)
y = np.sin(x) * 10  # some transformation to give varying y values

plt.plot(x, y)

y_min, y_max = y.min(), y.max()  # calculate min and max from y data
plt.ylim(y_min - 1, y_max + 1)  # add some padding for better visualization

plt.title("Automated Y-limits Example")
plt.xlabel("X-axis")
plt.ylabel("Y-axis")
plt.grid()
plt.show()


#Exercise 8



#Exercise 10
import pandas as pd

vocabulary = {0: 'the', 1: 'quick', 2: 'brown', 3: 'fox', 4: 'jumps', 
              5: 'over', 6: 'lazy', 7: 'dog', 8: 'and', 9: 'sleeps'}

data = {
    'Record': [
        [0, 1, 2],  # Example records
        [3, 4, 5],
        [6, 7, 8],
        [9, 0, 1],
        [1, 3, 1],  # The fifth record with two '1's
        [0, 2, 4]
    ]
}
X = pd.DataFrame(data)

fifth_record = X.iloc[4]['Record']  # Get the fifth record
print("Fifth Record:", fifth_record)

count_of_ones = fifth_record.count(1)

if count_of_ones >= 2:
    second_one_index = fifth_record.index(1, fifth_record.index(1) + 1)  # Find second occurrence
    print("Second occurrence of 1 found at index:", second_one_index)
    
    # Retrieve the word corresponding to the second '1' from the vocabulary
    word = vocabulary[1]  # Using 1 to fetch from vocabulary
    print("Word represented by the second '1':", word)
else:
    print("Less than two occurrences of '1' in the record.")

#Exercise 11
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.sparse import csr_matrix

terms = ['term1', 'term2', 'term3', 'term4', 'term5']
documents = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']
data = [
    [0, 1, 0, 0, 0],
    [1, 0, 0, 0, 1],
    [0, 0, 0, 1, 0],
    [0, 0, 1, 0, 0],
    [1, 0, 0, 1, 0],
]
term_doc_matrix = pd.DataFrame(data, columns=terms, index=documents)

sparse_matrix = csr_matrix(term_doc_matrix.values)

sample_rows = term_doc_matrix.sample(n=min(3, len(term_doc_matrix)), random_state=1)  # Adjust n for larger datasets
sample_columns = term_doc_matrix.columns[:3]  # Select first three columns for visualization

sample_matrix = sample_rows[sample_columns]

plt.figure(figsize=(10, 6))
sns.heatmap(sample_matrix, annot=True, cmap='viridis', cbar_kws={'label': 'Frequency'})
plt.title('Sample Term-Document Matrix')
plt.xlabel('Terms')
plt.ylabel('Documents')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
plt.hist(sample_matrix.values.flatten(), bins=5, alpha=0.7, color='blue', edgecolor='black')
plt.title('Histogram of Term Frequencies')
plt.xlabel('Frequency')
plt.ylabel('Count')
plt.show()

#Exercise 12 
import pandas as pd
import numpy as np
import plotly.express as px

terms = ['term1', 'term2', 'term3', 'term4', 'term5']
documents = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']
data = [
    [0, 1, 0, 0, 0],
    [1, 0, 0, 0, 1],
    [0, 0, 0, 1, 0],
    [0, 0, 1, 0, 0],
    [1, 0, 0, 1, 0],
]
term_doc_matrix = pd.DataFrame(data, columns=terms, index=documents)

term_doc_matrix_reset = term_doc_matrix.reset_index()
melted_matrix = term_doc_matrix_reset.melt(id_vars='index', var_name='Terms', value_name='Frequency')

fig = px.imshow(
    term_doc_matrix,
    labels=dict(x="Terms", y="Documents", color="Frequency"),
    x=terms,
    y=documents,
    color_continuous_scale='Viridis',
    title='Interactive Term-Document Matrix Heatmap'
)

fig.show()

#Exercise 13
import pandas as pd
import numpy as np
import plotly.express as px

terms = ['term1', 'term2', 'term3', 'term4', 'term5', 'term6', 'term7', 'term8', 'term9', 'term10']
documents = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5', 'doc6', 'doc7', 'doc8', 'doc9', 'doc10']
data = [
    [0, 1, 0, 2, 0, 3, 0, 0, 0, 1],
    [1, 0, 0, 0, 1, 0, 0, 0, 2, 1],
    [0, 0, 0, 1, 0, 2, 3, 1, 0, 0],
    [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
    [1, 0, 0, 1, 0, 0, 0, 0, 1, 0],
    [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
    [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
    [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],
    [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
]
term_doc_matrix = pd.DataFrame(data, columns=terms, index=documents)

term_frequencies = term_doc_matrix.sum().sort_values(ascending=False)

top_n = 5  # Set how many top terms you want to keep
top_terms = term_frequencies.head(top_n).index

filtered_matrix = term_doc_matrix[top_terms]

fig = px.imshow(
    filtered_matrix,
    labels=dict(x="Terms", y="Documents", color="Frequency"),
    x=top_terms,
    y=filtered_matrix.index,
    color_continuous_scale='Viridis',
    title='Interactive Heatmap of Top Terms in Term-Document Matrix'
)


# Exercise 15
import pandas as pd
import numpy as np
import plotly.express as px

terms = ['term1', 'term2', 'term3', 'term4', 'term5', 'term6', 'term7', 'term8', 'term9', 'term10']
documents = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5', 'doc6', 'doc7', 'doc8', 'doc9', 'doc10']
data = [
    [0, 1, 0, 2, 0, 3, 0, 0, 0, 1],
    [1, 0, 0, 0, 1, 0, 0, 0, 2, 1],
    [0, 0, 0, 1, 0, 2, 3, 1, 0, 0],
    [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
    [1, 0, 0, 1, 0, 0, 0, 0, 1, 0],
    [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
    [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
    [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],
    [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
]
term_doc_matrix = pd.DataFrame(data, columns=terms, index=documents)

term_frequencies = term_doc_matrix.sum().sort_values(ascending=False)

term_frequencies_log = np.log1p(term_frequencies)  # Use log1p to avoid log(0)

top_n = 5  # Set how many top terms you want to keep
top_terms_log = term_frequencies_log.head(top_n).index

filtered_matrix_log = term_doc_matrix[top_terms_log]

sorted_top_terms_log = term_frequencies_log.head(top_n).index

fig = px.imshow(
    filtered_matrix_log[sorted_top_terms_log],
    labels=dict(x="Terms", y="Documents", color="Log Frequency"),
    x=sorted_top_terms_log,
    y=filtered_matrix_log.index,
    color_continuous_scale='Viridis',
    title='Interactive Heatmap of Top Terms Sorted by Log Frequency'
)

fig.show()

#Exercise 17
import pandas as pd
import time
from pami import FAE_TopK, MaxFPGrowth

def load_comp_graphics_data():
    # Load your dataset and filter for 'comp.graphics'
    df = pd.read_csv('path/to/your/dataset.csv')
    comp_graphics_data = df[df['category'] == 'comp.graphics']
    return comp_graphics_data

def prepare_data_for_mining(comp_graphics_data):
    # Implement any necessary preprocessing to convert your data into a suitable format
    # For example, creating a list of transactions
    transactions = []  # Placeholder for the list of transactions
    for _, row in comp_graphics_data.iterrows():
        # Convert the document or terms into a transaction list
        transactions.append(row['text'].split())  # Example transformation
    return transactions

def run_fae_top_k(transactions, k_values):
    for k in k_values:
        start_time = time.time()
        fae_top_k = FAE_TopK(transactions, k)
        patterns = fae_top_k.run()  # Assuming run() method exists
        runtime = time.time() - start_time
        print(f'FAE Top-K (k={k}) runtime: {runtime:.2f}s')
        print("Patterns:", patterns)

def run_max_fp_growth(transactions, min_supports):
    for min_support in min_supports:
        start_time = time.time()
        max_fp_growth = MaxFPGrowth(transactions, min_support)
        patterns = max_fp_growth.run()  # Assuming run() method exists
        runtime = time.time() - start_time
        print(f'MaxFPGrowth (min_support={min_support}) runtime: {runtime:.2f}s')
        print("Patterns:", patterns)

if __name__ == "__main__":
    comp_graphics_data = load_comp_graphics_data()
    transactions = prepare_data_for_mining(comp_graphics_data)
    
    # Run FAE Top-K with k values
    k_values = [500, 1000, 1500]
    run_fae_top_k(transactions, k_values)
    
    # Run MaxFPGrowth with min support thresholds
    min_supports = [3, 6, 9]
    run_max_fp_growth(transactions, min_supports)



#Exercise 18
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

def load_data():
    # For demonstration, let's create a random dataset
    # Replace this with your actual dataset loading
    data = np.random.rand(100, 10)  # 100 samples with 10 features
    return data

def apply_pca(data):
    # Standardizing the data
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)

    pca = PCA(n_components=3)
    data_pca = pca.fit_transform(data_scaled)
    return data_pca

def plot_3d(data_pca):
    fig = plt.figure(figsize=(12, 8))

    ax = fig.add_subplot(111, projection='3d')

    ax.scatter(data_pca[:, 0], data_pca[:, 1], data_pca[:, 2], c='b', marker='o')

    ax.set_xlabel('PCA Component 1')
    ax.set_ylabel('PCA Component 2')
    ax.set_zlabel('PCA Component 3')
    ax.set_title('3D PCA Projection')

    for angle in [0, 90, 180]:
        ax.view_init(elev=20, azim=angle)
        plt.show()

if __name__ == "__main__":
    data = load_data()  # Load your dataset here
    data_pca = apply_pca(data)  # Reduce dimensions
    plot_3d(data_pca)  # Plot the results


 #Exercise 19
import pandas as pd

data = {
    'category_name': ['sci.med', 'sci.space', 'sci.med', 'sci.space', 'comp.graphics', 'sci.space'],
    'text': ['text1', 'text2', 'text3', 'text4', 'text5', 'text6']
}

df = pd.DataFrame(data)

print("Original DataFrame:")
print(df)

binarized_df = pd.get_dummies(df, columns=['category_name'], prefix='', prefix_sep='')

print("\nBinarized DataFrame (using get_dummies):")
print(binarized_df)







